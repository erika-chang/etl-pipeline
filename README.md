# ğŸš€ Simple Sales Data ETL Pipeline

## ğŸ“– Overview

This project is a simple ETL (Extract, Transform, Load) pipeline. It extracts sales and product data from a source SQL Server database, performs transformations to create an aggregated analytics table, and loads the result into a PostgreSQL database.

The goal is to demonstrate best practices in building a data pipeline, including a modular structure, logging, and clear documentation. The project is fully reproducible thanks to Docker for the PostgreSQL database and Python scripts for source data creation.

## âœ¨ Features

ğŸ³ Containerized Destination: Easily run PostgreSQL with Docker and Docker Compose.

ğŸ” Fully Reproducible: Scripts create and populate the source database from scratch.

ğŸ›  Modular Design: ETL is split into extract, transform, and load modules.

ğŸ“Š Meaningful Transformation: Joins fact and dimension tables into a denormalized analytics table.

ğŸ“œ Robust Logging: Tracks pipeline execution with Python's logging module.


## ğŸ“‚ Project Structure
```bash
etl-pipeline/
â”œâ”€â”€ README.md
â”œâ”€â”€ database_setup
â”‚Â Â  â”œâ”€â”€ data_gen.py
â”‚Â Â  â””â”€â”€ models.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ logs
â”œâ”€â”€ main.py
â”œâ”€â”€ pipeline
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ extract.py
â”‚Â Â  â”œâ”€â”€ load.py
â”‚Â Â  â””â”€â”€ transform.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ sql
    â””â”€â”€ create_schema.sql
```

## ğŸ›  Prerequisites

- Python 3.8+

- Docker & Docker Compose


## âš¡ How to Run

The pipeline has three main stages: Configuration â†’ Database Setup â†’ Run ETL.

### 1ï¸âƒ£ Initial Configuration

Clone the repository:

```bash
git clone <your-repo-url>
cd etl-pipeline
```

Create a virtual environment and install dependencies:

```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```


### 2ï¸âƒ£ Setting Up Databases

Start PostgreSQL container:

```bash
docker-compose up -d
```

Check if itâ€™s running:

```bash
docker ps
```

Populate source SQL Server database:

```bash
cd database_setup
python populate_database.py
cd ..
```

### 3ï¸âƒ£ Run the ETL Pipeline

```bash
python main.py
```

Logs appear in the console and in logs/etl_pipeline.log.

Final analytics data is stored in the PostgreSQL analytics schema.

### ğŸ›‘ Stopping the Environment

```bash
docker-compose down
```

Stops the container.

Data is preserved in the postgres_data volume.

## ğŸ“ˆ Key Outcome

âœ… Cleaned and transformed sales data in PostgreSQL.

âœ… Modular ETL code, easy to extend.

âœ… Fully reproducible setup using Docker.
